import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from numpy import Inf

# import VGG 19 model and keras Model API
from tensorflow.python.keras.applications.vgg19 import VGG19, preprocess_input
from tensorflow.python.keras.preprocessing.image import load_img, img_to_array
from tensorflow.python.keras.models import Model
# Image Credits: Tensorflow Doc
# Image Credits: Tensorflow Doc
content_path = tf.keras.utils.get_file('content.jpg',
         'D://Dog.jpg')
style_path = tf.keras.utils.get_file('style.jpg','D://styleimage.jpeg')

# code
# this function download the VGG model and initialise it
model = VGG19(
 include_top=False,
 weights='imagenet'
)
# set training to False
model.trainable = False
# Print details of different layers

model.summary()
# code to load and process image
def load_and_process_image(image_path):
 img = load_img(image_path)
 # convert image to array
 img = img_to_array(img)
 img = preprocess_input(img)
 img = np.expand_dims(img, axis=0)
 return img
# code
def deprocess(img):
 # perform the inverse of the pre processing step
 img[:, :, 0] += 103.939
 img[:, :, 1] += 116.779
 img[:, :, 2] += 123.68
 # convert RGB to BGR
 img = img[:, :, ::-1]

 img = np.clip(img, 0, 255).astype('uint8')
 return img



def display_image(image):
 # remove one dimension if image has 4 dimension
 if len(image.shape) == 4:
  img = np.squeeze(image, axis=0)

 img = deprocess(img)

 plt.grid(False)
 plt.xticks([])
 plt.yticks([])
 plt.imshow(img)
 return
# load content image
content_img = load_and_process_image(content_path)

# load style image
style_img = load_and_process_image(style_path)

# define content model
content_layer = 'block5_conv2'
content_model = Model(
 inputs=model.input,
 outputs=model.get_layer(content_layer).output
)
content_model.summary()
# define style model
style_layers = [
 'block1_conv1',
 'block3_conv1',
 'block5_conv1'
]
style_models = [Model(inputs=model.input,
     outputs=model.get_layer(layer).output) for layer in style_layers]
# Content loss
def content_loss(content, generated):
 a_C = content_model(content)
 loss = tf.reduce_mean(tf.square(a_C))
 return loss
# gram matrix
def gram_matrix(A):
 channels = int(A.shape[-1])
 a = tf.reshape(A, [-1, channels])
 n = tf.shape(a)[0]
 gram = tf.matmul(a, a, transpose_a=True)
 return gram / tf.cast(n, tf.float32)


weight_of_layer = 1. / len(style_models)


# style loss
def style_cost(style, generated):
 J_style = 0

 for style_model in style_models:
  a_S = style_model(style)
  a_G = style_model(generated)
  GS = gram_matrix(a_S)
  GG = gram_matrix(a_G)
  current_cost = tf.reduce_mean(tf.square(GS - GG))
  J_style += current_cost * weight_of_layer

 return J_style
# training function
import cv2
content_img = cv2.imread('D://Dog.jpg')
style_img = cv2.imread('D://styleimage.jpeg')
content_img = cv2.resize(content_img,(512,512))
style_img = cv2.resize(style_img,(512,512))
final_img = cv2.add(content_img,style_img)
cv2.imshow("neural style transfer",final_img)
cv2.waitKey(0)
cv2.destroyAllWindows()